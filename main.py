# -*- coding: utf-8 -*-
"""project3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qoi6c2kbvQye5fUrXBb3Oo9eE8RD3RUd
"""

#imports
import matplotlib.pyplot as plt
import numpy as np
import pickle
import tensorflow as tf
import keras
from keras import models
from keras.layers import Reshape,Dense,Dropout,Activation,Flatten
from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D
from sklearn.model_selection import train_test_split
from scipy import integrate

# !wget http://opendata.deepsig.io/datasets/2016.10/RML2016.10a.tar.bz2
# !tar -tvv RML2016.10a.tar.bz2
# !tar jxf RML2016.10a.tar.bz2


# !wget http://opendata.deepsig.io/datasets/2016.10/RML2016.10b.tar.bz2
# !tar xjf RML2016.10b.tar.bz2

# !ls                                        #check if needed files are presented  needed file ---->RML2016.10b.dat, sample_data

"""Load Dataset"""

#load dataset 1,200,000
file = open("RML2016.10b.dat",'rb')
Xd = pickle.load(file,encoding = 'bytes')
snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])
dataSet = []  
labelAndSNR = []
for mod in mods:
    for snr in snrs:
        dataSet.append(Xd[(mod,snr)])
        for i in range(Xd[(mod,snr)].shape[0]):  labelAndSNR.append((mod,snr))
dataSet = np.vstack(dataSet)
file.close()

classes = mods
numOfClasses=10

"""128 dimentions represent values during each micro second *so* these are samples in time
 Now we will create other feature spaces as:
1. Raw time series as given (two channels)
2. First derivative in time (two channels)
3. Integral in time (two channels)
"""

#First derivative in time, derivetave -> difference 
dataSetDriv = np.diff(dataSet)
#add lost dimension from difference
dataSetDriv = np.concatenate((np.zeros((1200000,2,1)),dataSetDriv),axis=2)

"""For the integration in time I tried all 4 Integrating functions for given fixed samples of integrate class to reach the suitable function:

cumtrapz -> returns array after integration not final value
"""

#Integral in time
#cumtrapz -> Cumulatively integrate y(x) using the composite trapezoidal rule
dataSetInt  = integrate.cumtrapz(dataSet)
dataSetInt = np.concatenate((np.zeros((1200000,2,1)),dataSetInt),axis=2)

"""Split data into train and test data"""

nExamples = dataSet.shape[0]
trainToTestSplit = 50
nTrain = nExamples * (trainToTestSplit/100)

trainIdx = np.random.choice(range(0,nExamples), size= int(nTrain), replace=False)
testIdx = list(set(range(0,nExamples))-set(trainIdx))

def multiClassClassify(yy):
    yy1 = np.zeros([len(yy), max(yy)+1])
    yy1[np.arange(len(yy)),yy] = 1
    return yy1 
labelsTrain = multiClassClassify(list(map(lambda x: mods.index(labelAndSNR[x][0]), trainIdx)))
labelsTest = multiClassClassify(list(map(lambda x: mods.index(labelAndSNR[x][0]), testIdx)))

"""Choose features to work on"""

#Raw time series
# dataSetTrain = dataSet[trainIdx]
# dataSetTest =  dataSet[testIdx]

#First derivative in time
# dataSetTrain = dataSetDriv[trainIdx]
# dataSetTest =  dataSetDriv[testIdx]

# Integral in time
dataSetTrain = dataSetInt[trainIdx]
dataSetTest =  dataSetInt[testIdx]

#reshape data
dataSetTrainShaped = list(dataSetTrain.shape[1:])

"""Setting parameters values"""

###############################################################################


#parameters
epoch = 100
batchSize = 2024


################################################################################

"""Fully connected neural network"""

#build fully connected neural network
model = models.Sequential()

#I/P
model.add(Reshape(dataSetTrainShaped+[1], input_shape=dataSetTrainShaped))
model.add(Flatten())


model.add(Dense(512, activation='relu', kernel_initializer='he_normal', name="layer1"))

model.add(Dense(512, activation='relu', kernel_initializer='he_normal', name="layer2"))

model.add(Dense(256, activation='relu', kernel_initializer='he_normal', name="layer3"))

model.add(Dense(256, activation='relu', kernel_initializer='he_normal', name="layer4"))

model.add(Dense(128, activation='relu', kernel_initializer='he_normal', name="layer5"))

model.add(Dense(128, activation='relu', kernel_initializer='he_normal', name="layer6"))

model.add(Dense(64, activation='relu', kernel_initializer='he_normal', name="layer7"))

model.add(Dense(64, activation='relu', kernel_initializer='he_normal', name="layer8"))



model.add(Dense(numOfClasses, activation='softmax', kernel_initializer='he_normal', name="finalLayer" ))
# model.add(Activation('softmax'))
#op
model.add(Reshape([numOfClasses]))

model.compile(loss='categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])
model.summary()

"""Train Fully Connected neural network"""

#train fully conmected NN
filepath = 'convmodrecnets_CNN2_0.5.wts.h5'
his= model.fit(dataSetTrain,
   labelsTrain,
    batch_size=batchSize,
    nb_epoch=epoch,
    verbose=2,
    validation_split=0.05,
    callbacks = [
        keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='auto'),
        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')
    ])
# we re-load the best weights once training is finished
model.load_weights(filepath)

#score
print(model.metrics_names)
score = model.evaluate(dataSetTest,labelsTest,verbose=0,batch_size=batchSize)
print(score)

"""CNN architecture"""

# model = models.Sequential()
# model.add(Reshape(dataSetTrainShaped+[1], input_shape=dataSetTrainShaped))
# model.add(ZeroPadding2D((0, 2)))

# model.add(Conv2D(256, 1, 3, border_mode='valid', activation="relu", name="conv1", init='glorot_uniform'))
# model.add(ZeroPadding2D((0, 2)))

# model.add(Conv2D(256, 2, 3, border_mode="valid", activation="relu", name="conv2", init='glorot_uniform'))
# model.add(ZeroPadding2D((0, 2)))

# model.add(Conv2D(80, 1, 3, border_mode="valid", activation="relu", name="conv3", init='glorot_uniform'))
# model.add(ZeroPadding2D((0, 2)))

# model.add(Conv2D(80, 1, 3, border_mode="valid", activation="relu", name="conv4", init='glorot_uniform'))
# model.add(Flatten())

# #128 cause error
# model.add(Dense(128, activation='softmax', init='he_normal', name="dense1"))

# model.add(Dense(numOfClasses, activation='softmax', init='he_normal', name="dense2" ))
# # model.add(Activation('softmax'))

# #op
# model.add(Reshape([numOfClasses]))
# model.compile(loss='categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])
# model.summary()
#########################################################################################################
# model = models.Sequential()
# model.add(Reshape(dataSetTrainShaped+[1], input_shape=dataSetTrainShaped))
# model.add(ZeroPadding2D((0, 2)))

# model.add(Conv2D(256, 1, 3, border_mode='valid', activation="relu", name="conv1", init='glorot_uniform'))
# model.add(ZeroPadding2D((0, 2)))

# model.add(Conv2D(80, 2, 3, border_mode="valid", activation="relu", name="conv2", init='glorot_uniform'))

# model.add(Conv2D(256, 1, 3, border_mode='valid', activation="relu", name="conv1", init='glorot_uniform'))
# model.add(Flatten())

# #128 cause error
# model.add(Dense(256, activation='relu', init='he_normal', name="dense1"))

# model.add(Dense(numOfClasses, init='he_normal', name="dense2" ))
# model.add(Activation('softmax'))

# #op
# model.add(Reshape([numOfClasses]))
# model.compile(loss='categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])
# model.summary()
#########################################################################################################
model = models.Sequential()
model.add(Reshape(dataSetTrainShaped+[1], input_shape=dataSetTrainShaped))
model.add(ZeroPadding2D((0, 2)))

model.add(Conv2D(64, 1, 3, border_mode='valid', activation="relu", name="conv1", init='glorot_uniform'))
model.add(ZeroPadding2D((0, 2)))

model.add(Conv2D(16, 2, 3, border_mode="valid", activation="relu", name="conv2", init='glorot_uniform'))
model.add(Flatten())

#128 cause error
model.add(Dense(256, activation='relu', init='he_normal', name="dense1"))

model.add(Dense(numOfClasses, init='he_normal', name="dense2" ))
model.add(Activation('softmax'))

#op
model.add(Reshape([numOfClasses]))
model.compile(loss='categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])
model.summary()

"""Train CNN architecture"""

#train CNN
filepath = 'convmodrecnets_CNN2_0.5.wts.h5'
his= model.fit(dataSetTrain,
   labelsTrain,
    batch_size=batchSize,
    nb_epoch=epoch,
    verbose=2,
    validation_split=0.05,
    callbacks = [
        keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='auto'),
        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')
    ])
# we re-load the best weights once training is finished
model.load_weights(filepath)

#score
print(model.metrics_names)
score = model.evaluate(dataSetTest,labelsTest,verbose=0,batch_size=batchSize)
print(score)

"""---


OPS
---

Confussion matrix
"""

def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(labels))
    plt.xticks(tick_marks, labels, rotation=45)
    plt.yticks(tick_marks, labels)
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

# Plot general confusion matrix
test_Y_hat = model.predict(dataSetTest, batch_size=batchSize)
conf = np.zeros([len(classes),len(classes)])
confnorm = np.zeros([len(classes),len(classes)])
for i in range(0,dataSetTest.shape[0]):
    j = list(labelsTest[i,:]).index(1)
    k = int(np.argmax(test_Y_hat[i,:]))
    conf[j,k] = conf[j,k] + 1
for i in range(0,len(classes)):
    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])
plot_confusion_matrix(confnorm, labels=classes)

# Plot confusion matrix for different SNRs
acc = {}
for snr in snrs:

    # extract classes @ SNR
    test_SNRs = list(map(lambda x: labelAndSNR[x][1], testIdx))
    test_X_i = dataSetTest[np.where(np.array(test_SNRs)==snr)]
    test_Y_i = labelsTest[np.where(np.array(test_SNRs)==snr)]    

    # estimate classes
    test_Y_i_hat = model.predict(test_X_i)
    conf = np.zeros([len(classes),len(classes)])
    confnorm = np.zeros([len(classes),len(classes)])
    for i in range(0,test_X_i.shape[0]):
        j = list(test_Y_i[i,:]).index(1)
        k = int(np.argmax(test_Y_i_hat[i,:]))
        conf[j,k] = conf[j,k] + 1
    for i in range(0,len(classes)):
        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])
    plt.figure()
    plot_confusion_matrix(confnorm, labels=classes, title="ConvNet Confusion Matrix (SNR=%d)"%(snr))
    
    cor = np.sum(np.diag(conf))
    ncor = np.sum(conf) - cor
    print ("Overall Accuracy at SNR",snr," =", cor / (cor+ncor))
    acc[snr] = 1.0*cor/(cor+ncor)

"""Accuracy Curve"""

# Plot accuracy curve
plt.plot(snrs, list(map(lambda x: acc[x], snrs)))
plt.xlabel("Signal to Noise Ratio")
plt.ylabel("Classification Accuracy")
plt.title("Classification Accuracy on RadioML 2016.10B")